{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Memory-efficient PDF-to-Image conversion.\n",
    "\"\"\"\n",
    "\n",
    "import subprocess as sp\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PdfImages:\n",
    "    \"\"\"\n",
    "    A container that holds images of PDF pages. It is a generic python sequence that\n",
    "    supports `len()` and indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, dpi: int = 200, password: str = \"\"):\n",
    "        \"\"\"\n",
    "        Converts all pages to images and stores them in a temporary directory.\n",
    "\n",
    "        Args:\n",
    "            path: str\n",
    "                Path to PDF file\n",
    "            dpi: int\n",
    "                Dots per inch (quality of image)\n",
    "            password: str\n",
    "                Password to decrypt encrypted PDFs\n",
    "        \"\"\"\n",
    "        self.tempdir = tempfile.TemporaryDirectory()\n",
    "        tempdir_name = self.tempdir.name\n",
    "        sp.run(\n",
    "            [\n",
    "                \"pdftoppm\",\n",
    "                str(path),\n",
    "                f\"{tempdir_name}/out\",\n",
    "                \"-jpeg\",\n",
    "                \"-r\",\n",
    "                str(dpi),\n",
    "                \"-upw\",\n",
    "                str(password),\n",
    "                \"-opw\",\n",
    "                str(password),\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        self.images = sorted(glob(f\"{tempdir_name}/*.jpg\"))\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Removes the temporary directory where the page-wise images are stored.\"\"\"\n",
    "        self.tempdir.cleanup()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of pages.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int) -> np.ndarray:\n",
    "        \"\"\"Returns the image of the particular page/index.\"\"\"\n",
    "        return cv.imread(self.images[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vision_image(image):\n",
    "    \"\"\"\n",
    "    Construct Google Vision Image object from various types of input.\n",
    "    \"\"\"\n",
    "    # check if path\n",
    "    # try:\n",
    "    ext = Path(image).suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        image = PdfImages(image)[0]\n",
    "        content = cv2.imencode(\".jpg\", image)[1].tostring()\n",
    "        return vision.Image(content=content)\n",
    "    elif ext in [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\", \".tiff\"]:\n",
    "        content = io.open(image, \"rb\").read()\n",
    "        return vision.Image(content=content)\n",
    "    # except (AttributeError, OSError):\n",
    "    #     print(\"Invalid image path\")\n",
    "    #     print_traceback()\n",
    "\n",
    "    # check if PIL\n",
    "    if isinstance(image, Image.Image):\n",
    "        with io.BytesIO() as buffer:\n",
    "            image.save(buffer, format=\"JPEG\")\n",
    "            return vision.types.Image(content=buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k= _get_image(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example\n",
    "pdf_path = \"/Users/avinash/Desktop/Personal projects/ocr_to_layout-text/test_dataset/Demo Documents/Freight Invoice/1270802_EMLM_CROWN_IMPORTS_LLC-1.pdf\"\n",
    "# image = construct_vision_image(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_structure.utils import _get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = _get_image(pdf_path)\n",
    "image = vision.Image(content=k[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()\n",
    "response =client.document_text_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = []\n",
    "# lines = {}\n",
    "# y_threshold = 30  # Set the threshold value for y-axis alignment\n",
    "\n",
    "# for text in response.text_annotations[1:]:\n",
    "#     top_x_axis = text.bounding_poly.vertices[0].x\n",
    "#     top_y_axis = text.bounding_poly.vertices[0].y\n",
    "#     bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "\n",
    "#     # Find an existing line that this text could belong to\n",
    "#     found_line = None\n",
    "#     for s_top_y_axis, s_item in lines.items():\n",
    "#         if abs(s_top_y_axis - top_y_axis) <= y_threshold:\n",
    "#             if top_y_axis < s_item[0][1] + y_threshold:\n",
    "#                 found_line = s_top_y_axis\n",
    "#                 break\n",
    "\n",
    "#     if found_line is None:\n",
    "#         # No suitable line found, create a new line\n",
    "#         lines[top_y_axis] = [(top_y_axis, bottom_y_axis), []]\n",
    "#         found_line = top_y_axis\n",
    "#     else:\n",
    "#         # Update the bottom_y_axis if necessary\n",
    "#         _, current_bottom_y = lines[found_line][0]\n",
    "#         if bottom_y_axis > current_bottom_y:\n",
    "#             lines[found_line][0] = (top_y_axis, bottom_y_axis)\n",
    "\n",
    "#     # Add the text to the found line\n",
    "#     lines[found_line][1].append((top_x_axis, text.description))\n",
    "\n",
    "# # Sort and join the texts for each line with adjusted spacing\n",
    "# for _, item in lines.items():\n",
    "#     if item[1]:\n",
    "#         words = sorted(item[1], key=lambda t: t[0])\n",
    "#         sentence = []\n",
    "#         last_x = None\n",
    "#         for x, word in words:\n",
    "#             if last_x is not None:\n",
    "#                 # Calculate space width based on the difference in x positions\n",
    "#                 space_width = int((x - last_x) / 30)  # Adjust divisor to scale space width\n",
    "#                 sentence.append(' ' * max(space_width, 1) + word)\n",
    "#             else:\n",
    "#                 sentence.append(word)\n",
    "#             last_x = x + len(word) * 7  # Estimate the end x position of the current word\n",
    "\n",
    "#         items.append((item[0], ''.join(sentence), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = []\n",
    "# lines = {}\n",
    "# y_threshold = 20  # Set the threshold value for y-axis alignment\n",
    "# x_segment_width = 10  # Define the width of each horizontal segment\n",
    "# max_segment_length = 50  # Maximum characters in a segment before text wraps to a new line\n",
    "\n",
    "# for text in response.text_annotations[1:]:\n",
    "#     top_x_axis = text.bounding_poly.vertices[0].x\n",
    "#     top_y_axis = text.bounding_poly.vertices[0].y\n",
    "#     bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "\n",
    "#     # Find an existing line that this text could belong to\n",
    "#     found_line = None\n",
    "#     for s_top_y_axis, s_item in lines.items():\n",
    "#         if abs(s_top_y_axis - top_y_axis) <= y_threshold:\n",
    "#             if top_y_axis < s_item[0][1] + y_threshold:\n",
    "#                 found_line = s_top_y_axis\n",
    "#                 break\n",
    "\n",
    "#     if found_line is None:\n",
    "#         # No suitable line found, create a new line\n",
    "#         lines[top_y_axis] = [(top_y_axis, bottom_y_axis), {}]\n",
    "#         found_line = top_y_axis\n",
    "#     else:\n",
    "#         # Update the bottom_y_axis if necessary\n",
    "#         _, current_bottom_y = lines[found_line][0]\n",
    "#         if bottom_y_axis > current_bottom_y:\n",
    "#             lines[found_line][0] = (top_y_axis, bottom_y_axis)\n",
    "\n",
    "#     # Determine the segment for the text based on the x-coordinate\n",
    "#     segment_index = top_x_axis // x_segment_width\n",
    "#     if segment_index not in lines[found_line][1]:\n",
    "#         lines[found_line][1][segment_index] = []\n",
    "\n",
    "#     # Add the text to the appropriate segment\n",
    "#     lines[found_line][1][segment_index].append((top_x_axis, text.description))\n",
    "\n",
    "# # Sort and join the texts for each line and segment, handling overflow\n",
    "# for _, item in lines.items():\n",
    "#     sorted_segments = sorted(item[1].items())\n",
    "#     full_line = []\n",
    "#     last_segment_end = 0\n",
    "#     for segment_index, words in sorted_segments:\n",
    "#         segment_start = segment_index * x_segment_width\n",
    "#         if segment_start > last_segment_end:\n",
    "#             full_line.append(' ' * ((segment_start - last_segment_end) // 7))\n",
    "#         sorted_words = sorted(words, key=lambda t: t[0])\n",
    "#         segment_text = ' '.join(word for _, word in sorted_words)\n",
    "        \n",
    "#         # Check for overflow and handle by adding new lines\n",
    "#         while len(segment_text) > max_segment_length:\n",
    "#             cut_point = segment_text.rfind(' ', 0, max_segment_length)\n",
    "#             if cut_point == -1:  # No space found, force cut\n",
    "#                 cut_point = max_segment_length\n",
    "#             full_line.append(segment_text[:cut_point])\n",
    "#             full_line.append('\\n' + ' ' * (segment_start // 7))  # New line with indentation\n",
    "#             segment_text = segment_text[cut_point:].strip()\n",
    "        \n",
    "#         full_line.append(segment_text)\n",
    "#         last_segment_end = segment_start + len(segment_text) * 7\n",
    "\n",
    "#     items.append((item[0], ''.join(full_line)))\n",
    "\n",
    "# # Output the formatted text\n",
    "# for item in items:\n",
    "#     print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = []\n",
    "# lines = {}\n",
    "# y_threshold = 20  # Set the threshold value for y-axis alignment\n",
    "# num_segments = 20  # Number of horizontal segments\n",
    "# max_x_coordinate = max(text.bounding_poly.vertices[1].x for text in response.text_annotations[1:])  # Find the maximum x-coordinate\n",
    "# segment_width = max_x_coordinate // num_segments  # Calculate segment width dynamically\n",
    "# last_bottom_y = 0  # Track the bottom y-coordinate of the last processed line\n",
    "# x_close_threshold = 120  # Threshold for x-axis closeness to consider merging words into the same block\n",
    "\n",
    "# # Additional structure to track the last word's x-coordinate in each line\n",
    "# last_word_x = {}\n",
    "\n",
    "# for text in response.text_annotations[1:]:\n",
    "#     top_x_axis = text.bounding_poly.vertices[0].x\n",
    "#     top_y_axis = text.bounding_poly.vertices[0].y\n",
    "#     bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "\n",
    "#     # Find an existing line that this text could belong to\n",
    "#     found_line = None\n",
    "#     for s_top_y_axis, s_item in lines.items():\n",
    "#         if abs(s_top_y_axis - top_y_axis) <= y_threshold:\n",
    "#             if top_y_axis < s_item[0][1] + y_threshold:\n",
    "#                 found_line = s_top_y_axis\n",
    "#                 break\n",
    "\n",
    "#     if found_line is None:\n",
    "#         # No suitable line found, create a new line\n",
    "#         lines[top_y_axis] = [(top_y_axis, bottom_y_axis), {}]\n",
    "#         found_line = top_y_axis\n",
    "#         last_word_x[found_line] = 0  # Initialize the last word x-coordinate for this line\n",
    "#     else:\n",
    "#         # Update the bottom_y_axis if necessary\n",
    "#         _, current_bottom_y = lines[found_line][0]\n",
    "#         if bottom_y_axis > current_bottom_y:\n",
    "#             lines[found_line][0] = (top_y_axis, bottom_y_axis)\n",
    "\n",
    "#     # Determine the segment for the text based on the x-coordinate\n",
    "#     segment_index = top_x_axis // segment_width\n",
    "\n",
    "#     # Check if the word is too close to the last word in the same line\n",
    "#     if (top_x_axis - last_word_x[found_line]) < x_close_threshold:\n",
    "#         # Find the segment of the last word in the same line\n",
    "#         for seg_index, words in lines[found_line][1].items():\n",
    "#             if words and words[-1][0] == last_word_x[found_line]:\n",
    "#                 segment_index = seg_index  # Adjust the segment index to the last word's segment\n",
    "#                 break\n",
    "\n",
    "#     if segment_index not in lines[found_line][1]:\n",
    "#         lines[found_line][1][segment_index] = []\n",
    "\n",
    "#     # Add the text to the appropriate segment\n",
    "#     lines[found_line][1][segment_index].append((top_x_axis, text.description))\n",
    "#     last_word_x[found_line] = top_x_axis  # Update the last word x-coordinate for this line\n",
    "\n",
    "# # Sort and join the texts for each line and segment\n",
    "# sorted_lines = sorted(lines.items())\n",
    "# for top_y_axis, item in sorted_lines:\n",
    "#     sorted_segments = sorted(item[1].items())\n",
    "#     full_line = []\n",
    "#     last_segment_end = 0\n",
    "#     for segment_index, words in sorted_segments:\n",
    "#         segment_start = segment_index * segment_width\n",
    "#         if segment_start > last_segment_end and (segment_start - last_segment_end) // 7 > 1:\n",
    "#             full_line.append(' ' * ((segment_start - last_segment_end) // 7))\n",
    "#         sorted_words = sorted(words, key=lambda t: t[0])\n",
    "#         line_text = ' '.join(word for _, word in sorted_words)\n",
    "#         full_line.append(line_text)\n",
    "#         last_segment_end = segment_start + len(line_text) * 7\n",
    "\n",
    "#     items.append((item[0], ''.join(full_line)))\n",
    "\n",
    "#     # Check for large vertical gaps and add an extra line if necessary\n",
    "#     if last_bottom_y and (top_y_axis - last_bottom_y > y_threshold * 2):\n",
    "#         items.append((last_bottom_y + y_threshold, ''))\n",
    "#     last_bottom_y = item[0][1]\n",
    "\n",
    "# # Output the formatted text\n",
    "# for item in items:\n",
    "#     print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_from_annotations(response,exclusion_zones):\n",
    "    items = []\n",
    "    lines = {}\n",
    "    table_annotated = {}\n",
    "    y_threshold = 10  # Set the threshold value for y-axis alignment\n",
    "    num_segments = 30  # Number of horizontal segments\n",
    "    max_x_coordinate = max(text.bounding_poly.vertices[1].x for text in response.text_annotations[1:])  # Find the maximum x-coordinate\n",
    "    segment_width = max_x_coordinate // num_segments  # Calculate segment width dynamically\n",
    "    last_bottom_y = 0  # Track the bottom y-coordinate of the last processed line\n",
    "    x_close_threshold = 50  # Threshold for x-axis closeness to consider merging words into the same block\n",
    "\n",
    "    # Additional structure to track the last word's x-coordinate in each line\n",
    "    last_word_x = {}\n",
    "    def is_within_exclusion_zone(x1, y1, x2, y2):\n",
    "        for idx,ex_zone in exclusion_zones.items():\n",
    "            (ex_x1, ex_y1, ex_x2, ex_y2) = ex_zone\n",
    "            if not (x2 < ex_x1 or x1 > ex_x2 or y2 < ex_y1 or y1 > ex_y2):\n",
    "                return (True,idx)\n",
    "        return (False,False)\n",
    "\n",
    "    for text in response.text_annotations[1:]:\n",
    "        top_x_axis = text.bounding_poly.vertices[0].x\n",
    "        top_y_axis = text.bounding_poly.vertices[0].y\n",
    "        bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "        is_present = is_within_exclusion_zone(top_x_axis, top_y_axis, text.bounding_poly.vertices[1].x, bottom_y_axis)\n",
    "        if is_present[0]:\n",
    "            if is_present[1] not in table_annotated:\n",
    "                table_annotated[is_present[1]] = 1\n",
    "                text.description = len(text.description) * \" \"\n",
    "            else:\n",
    "                if table_annotated[is_present[1]] == 3:\n",
    "                    text.description = f\"# Table_{is_present[1]} is here\"\n",
    "                    table_annotated[is_present[1]] += 1\n",
    "                else:\n",
    "                    text.description = len(text.description) * \" \"\n",
    "                    table_annotated[is_present[1]] += 1\n",
    "\n",
    "        # Find an existing line that this text could belong to\n",
    "        found_line = None\n",
    "        for s_top_y_axis, s_item in lines.items():\n",
    "            if abs(s_top_y_axis - top_y_axis) <= y_threshold:\n",
    "                if top_y_axis < s_item[0][1] + y_threshold:\n",
    "                    found_line = s_top_y_axis\n",
    "                    break\n",
    "        print(table_annotated)\n",
    "        if found_line is None:\n",
    "            # No suitable line found, create a new line\n",
    "            lines[top_y_axis] = [(top_y_axis, bottom_y_axis), {}]\n",
    "            found_line = top_y_axis\n",
    "            last_word_x[found_line] = 0  # Initialize the last word x-coordinate for this line\n",
    "        else:\n",
    "            # Update the bottom_y_axis if necessary\n",
    "            _, current_bottom_y = lines[found_line][0]\n",
    "            if bottom_y_axis > current_bottom_y:\n",
    "                lines[found_line][0] = (top_y_axis, bottom_y_axis)\n",
    "        # Determine the segment for the text based on the x-coordinate\n",
    "        segment_index = top_x_axis // segment_width\n",
    "\n",
    "        # Check if the word is too close to the last word in the same line\n",
    "        if (top_x_axis - last_word_x[found_line]) < x_close_threshold:\n",
    "            # Find the segment of the last word in the same line\n",
    "            for seg_index, words in lines[found_line][1].items():\n",
    "                if words and words[-1][0] == last_word_x[found_line]:\n",
    "                    segment_index = seg_index  # Adjust the segment index to the last word's segment\n",
    "                    break\n",
    "\n",
    "        if segment_index not in lines[found_line][1]:\n",
    "            lines[found_line][1][segment_index] = []\n",
    "\n",
    "        # Add the text to the appropriate segment\n",
    "        lines[found_line][1][segment_index].append((top_x_axis, text.description))\n",
    "        last_word_x[found_line] = top_x_axis  # Update the last word x-coordinate for this line\n",
    "\n",
    "    # Sort and join the texts for each line and segment\n",
    "    sorted_lines = sorted(lines.items())\n",
    "    for top_y_axis, item in sorted_lines:\n",
    "        sorted_segments = sorted(item[1].items())\n",
    "        full_line = []\n",
    "        last_segment_end = 0\n",
    "        for segment_index, words in sorted_segments:\n",
    "            segment_start = segment_index * segment_width\n",
    "            if segment_start > last_segment_end and (segment_start - last_segment_end) // 7 > 1:\n",
    "                full_line.append(' ' * ((segment_start - last_segment_end) // 7))\n",
    "            sorted_words = sorted(words, key=lambda t: t[0])\n",
    "            line_text = ' '.join(word for _, word in sorted_words)\n",
    "            full_line.append(line_text)\n",
    "            last_segment_end = segment_start + len(line_text) * 7\n",
    "\n",
    "        items.append((item[0], ''.join(full_line)))\n",
    "\n",
    "        # Check for large vertical gaps and add an extra line if necessary\n",
    "        if last_bottom_y and (top_y_axis - last_bottom_y > y_threshold * 2):\n",
    "            items.append((last_bottom_y + y_threshold, ''))\n",
    "        last_bottom_y = item[0][1]\n",
    "\n",
    "    # Combine all lines into a single string\n",
    "    formatted_text = '\\n'.join(item[1] for item in items)\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{'1': 1}\n",
      "{'1': 2}\n",
      "{'1': 3}\n",
      "{'1': 4}\n",
      "{'1': 5}\n",
      "{'1': 6}\n",
      "{'1': 7}\n",
      "{'1': 7}\n",
      "{'1': 7}\n",
      "{'1': 7}\n",
      "{'1': 8}\n",
      "{'1': 9}\n",
      "{'1': 10}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 11}\n",
      "{'1': 12}\n",
      "{'1': 12, '2': 1}\n",
      "{'1': 12, '2': 2}\n",
      "{'1': 12, '2': 3}\n",
      "{'1': 13, '2': 3}\n",
      "{'1': 13, '2': 4}\n",
      "{'1': 13, '2': 5}\n",
      "{'1': 13, '2': 6}\n",
      "{'1': 13, '2': 7}\n",
      "{'1': 13, '2': 8}\n",
      "{'1': 13, '2': 9}\n",
      "{'1': 13, '2': 10}\n",
      "{'1': 13, '2': 11}\n",
      "{'1': 13, '2': 12}\n",
      "{'1': 13, '2': 13}\n",
      "{'1': 13, '2': 14}\n",
      "{'1': 13, '2': 15}\n",
      "{'1': 13, '2': 16}\n",
      "{'1': 13, '2': 17}\n",
      "{'1': 13, '2': 18}\n",
      "{'1': 13, '2': 19}\n",
      "{'1': 13, '2': 20}\n",
      "{'1': 13, '2': 21}\n",
      "{'1': 13, '2': 22}\n",
      "{'1': 13, '2': 23}\n",
      "{'1': 13, '2': 24}\n",
      "{'1': 13, '2': 25}\n",
      "{'1': 13, '2': 26}\n",
      "{'1': 13, '2': 27}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28}\n",
      "{'1': 13, '2': 28, '3': 1}\n",
      "{'1': 13, '2': 28, '3': 2}\n",
      "{'1': 13, '2': 28, '3': 3}\n",
      "{'1': 13, '2': 28, '3': 4}\n",
      "{'1': 13, '2': 28, '3': 5}\n",
      "{'1': 13, '2': 28, '3': 6}\n",
      "{'1': 13, '2': 28, '3': 7}\n",
      "{'1': 13, '2': 28, '3': 8}\n",
      "{'1': 13, '2': 28, '3': 9}\n",
      "{'1': 13, '2': 28, '3': 10}\n",
      "{'1': 13, '2': 28, '3': 11}\n",
      "{'1': 13, '2': 28, '3': 12}\n",
      "{'1': 13, '2': 28, '3': 13}\n",
      "{'1': 13, '2': 28, '3': 14}\n",
      "{'1': 13, '2': 28, '3': 15}\n",
      "{'1': 13, '2': 28, '3': 16}\n",
      "{'1': 13, '2': 28, '3': 17}\n",
      "{'1': 13, '2': 28, '3': 18}\n",
      "{'1': 13, '2': 28, '3': 19}\n",
      "{'1': 13, '2': 28, '3': 20}\n",
      "{'1': 13, '2': 28, '3': 21}\n",
      "{'1': 13, '2': 28, '3': 22}\n",
      "{'1': 13, '2': 28, '3': 23}\n",
      "{'1': 13, '2': 28, '3': 24}\n",
      "{'1': 13, '2': 28, '3': 25}\n",
      "{'1': 13, '2': 28, '3': 26}\n",
      "{'1': 13, '2': 28, '3': 27}\n",
      "{'1': 13, '2': 28, '3': 28}\n",
      "{'1': 13, '2': 28, '3': 29}\n",
      "{'1': 13, '2': 28, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 30}\n",
      "{'1': 13, '2': 29, '3': 31}\n",
      "{'1': 13, '2': 29, '3': 32}\n",
      "{'1': 13, '2': 29, '3': 33}\n",
      "{'1': 13, '2': 29, '3': 34}\n",
      "{'1': 13, '2': 29, '3': 35}\n",
      "{'1': 13, '2': 29, '3': 36}\n",
      "{'1': 13, '2': 29, '3': 37}\n",
      "{'1': 13, '2': 29, '3': 38}\n",
      "{'1': 13, '2': 29, '3': 39}\n",
      "{'1': 13, '2': 29, '3': 40}\n",
      "{'1': 13, '2': 29, '3': 41}\n",
      "{'1': 13, '2': 29, '3': 42}\n",
      "{'1': 13, '2': 29, '3': 43}\n",
      "{'1': 13, '2': 29, '3': 44}\n",
      "{'1': 13, '2': 29, '3': 45}\n",
      "{'1': 13, '2': 29, '3': 46}\n",
      "{'1': 13, '2': 29, '3': 47}\n",
      "{'1': 13, '2': 29, '3': 48}\n",
      "{'1': 13, '2': 29, '3': 49}\n",
      "{'1': 13, '2': 29, '3': 50}\n",
      "{'1': 13, '2': 29, '3': 51}\n",
      "{'1': 13, '2': 29, '3': 52}\n",
      "{'1': 13, '2': 29, '3': 53}\n",
      "{'1': 13, '2': 29, '3': 54}\n",
      "{'1': 13, '2': 29, '3': 55}\n",
      "{'1': 13, '2': 29, '3': 56}\n",
      "{'1': 13, '2': 29, '3': 57}\n",
      "{'1': 13, '2': 29, '3': 58}\n",
      "{'1': 13, '2': 29, '3': 59}\n",
      "{'1': 13, '2': 29, '3': 60}\n",
      "{'1': 13, '2': 29, '3': 61}\n",
      "{'1': 13, '2': 29, '3': 62}\n",
      "{'1': 13, '2': 29, '3': 63}\n",
      "{'1': 13, '2': 29, '3': 64}\n",
      "{'1': 13, '2': 29, '3': 65}\n",
      "{'1': 13, '2': 29, '3': 66}\n",
      "{'1': 13, '2': 29, '3': 67}\n",
      "{'1': 13, '2': 29, '3': 68}\n",
      "{'1': 13, '2': 29, '3': 69}\n",
      "{'1': 13, '2': 29, '3': 70}\n",
      "{'1': 13, '2': 29, '3': 71}\n",
      "{'1': 13, '2': 29, '3': 72}\n",
      "{'1': 13, '2': 29, '3': 73}\n",
      "{'1': 13, '2': 29, '3': 74}\n",
      "{'1': 13, '2': 29, '3': 75}\n",
      "{'1': 13, '2': 29, '3': 76}\n",
      "{'1': 13, '2': 29, '3': 77}\n",
      "{'1': 13, '2': 29, '3': 78}\n",
      "{'1': 13, '2': 29, '3': 79}\n",
      "{'1': 13, '2': 29, '3': 80}\n",
      "{'1': 13, '2': 29, '3': 81}\n",
      "{'1': 13, '2': 29, '3': 82}\n",
      "{'1': 13, '2': 29, '3': 83}\n",
      "{'1': 13, '2': 29, '3': 84}\n",
      "{'1': 13, '2': 29, '3': 85}\n",
      "{'1': 13, '2': 29, '3': 86}\n",
      "{'1': 13, '2': 29, '3': 87}\n",
      "{'1': 13, '2': 29, '3': 88}\n",
      "{'1': 13, '2': 29, '3': 89}\n",
      "{'1': 13, '2': 29, '3': 90}\n",
      "{'1': 13, '2': 29, '3': 91}\n",
      "{'1': 13, '2': 29, '3': 92}\n",
      "{'1': 13, '2': 29, '3': 93}\n",
      "{'1': 13, '2': 29, '3': 94}\n",
      "{'1': 13, '2': 29, '3': 95}\n",
      "{'1': 13, '2': 29, '3': 96}\n",
      "{'1': 13, '2': 29, '3': 97}\n",
      "{'1': 13, '2': 29, '3': 98}\n",
      "{'1': 13, '2': 29, '3': 99}\n",
      "{'1': 13, '2': 29, '3': 100}\n",
      "{'1': 13, '2': 29, '3': 101}\n",
      "{'1': 13, '2': 29, '3': 102}\n",
      "{'1': 13, '2': 29, '3': 103}\n",
      "{'1': 13, '2': 29, '3': 104}\n",
      "{'1': 13, '2': 29, '3': 105}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 106}\n",
      "{'1': 13, '2': 29, '3': 107}\n",
      "{'1': 13, '2': 29, '3': 108}\n",
      "{'1': 13, '2': 29, '3': 109}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "{'1': 13, '2': 29, '3': 110}\n",
      "Matson\n",
      "                                           ORIGINALFREIGHTBILL                              1 of 1\n",
      "                                                     # Table_1 is here                                                                      \n",
      "                                                                                                                         \n",
      "  Mail To :                                                                                                         \n",
      "                                           # Table_2 is here                                   \n",
      "  CROWN IMPORTS LLC                                                                                                                                                                                                                         \n",
      "  ATT : TONY ADAMS                                                                                              \n",
      "  131 SOUTH DEARBORN SUITE 1200                                                             \n",
      "  CHICAGO , IL 60603\n",
      "                                                                                    \n",
      "\n",
      "  SHPR : CROWN IMPORTS LLC     C / O BIAGI BROTHERS       ONTARIO , CA\n",
      "\n",
      "  CNEE : PARADISE BEVERAGES    94-1450 MOANIANI STREET    WAIPAHU , HI\n",
      "  BILL : CROWN IMPORTS LLC     ATTN TONY ADAMS            CHICAGO , IL\n",
      "                   # Table_3 is here                                                                                                        \n",
      "                                                                                                                                               \n",
      "                                                                                                                                                                                                                            \n",
      "                                    \n",
      "                                                                                          \n",
      "                                                                                                                                           \n",
      "                                                                                                                                           \n",
      "                                                                                                                                           \n",
      "                                                                                                                                            \n",
      "                                                                                                                                                                  \n",
      "\n",
      "                                                                                                                                                                                                                                                                                               \n",
      "                                    \n",
      "                                                                                                                                                                                                                                                                                                                                           \n",
      "\n",
      "                                                                                                                                                              \n",
      "                                                      * CREDIT VALIDATED*\n",
      "\n",
      "                                                      PLEASE RETURN THIS PORTION WITH YOUR REMITTANCE\n",
      "     SHIPMENT : 1804244-000                             PAYABLE UPON PRESENTATION\n",
      "                                                         PLEASE\n",
      "     PAYOR :                                         PAY THIS AMOUNT    PREPAID $ 4,447.18\n",
      "     REMIT TO : MATSON NAVIGATIONCOMPANY , INC .\n",
      "\n",
      "             P. O. BOX 98481 ,\n",
      "             CHICAGO , IL 60693\n",
      "                                                      Questions Service Representative concerning this at shipment 1-800-4 - MATSON , please call . Free your time Customer and\n",
      "                                                      charges in accordance for storage with applicable and equipment Matson detention local terminal will be tariffs determined .\n"
     ]
    }
   ],
   "source": [
    "print(format_text_from_annotations(response,{\"1\":(259, 58, 575, 98),\"2\":(259, 98, 575, 229),\"3\":(24, 280, 575, 586)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "\n",
    "def string_to_pdf(text, width=1800, height=1500):  # Default size for A4\n",
    "    # Create a new PDF with custom dimensions\n",
    "    pdfmetrics.registerFont(TTFont('Monaco', 'MONACO.TTF')) \n",
    "    c = canvas.Canvas(\"output.pdf\", pagesize=(width, height))\n",
    "    \n",
    "    c.setFont(\"Monaco\", 12)\n",
    "    # Set the font and size\n",
    "    # c.setFont(\"Courier New'\", 12)\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Draw each line on the PDF\n",
    "    y = height - 50  # Start drawing 50 points from the top of the page\n",
    "    for line in lines:\n",
    "        c.drawString(50, y, line)  # Start drawing 50 points from the left of the page\n",
    "        y -= 20  # Move down 20 points for the next line\n",
    "    \n",
    "    # Save the PDF\n",
    "    c.showPage()\n",
    "    c.save()\n",
    "\n",
    "text = \"\"\n",
    "for item in items:\n",
    "    text += item[1] + '\\n'\n",
    "string_to_pdf(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = response.full_text_annotation.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# items = []\n",
    "# lines = {}\n",
    "# y_threshold = 25  # Set the threshold value for y-axis alignment\n",
    "# x_positions = defaultdict(list)\n",
    "\n",
    "# for text in response.text_annotations[1:]:\n",
    "#     top_x_axis = text.bounding_poly.vertices[0].x\n",
    "#     top_y_axis = text.bounding_poly.vertices[0].y\n",
    "#     bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "\n",
    "#     # Find an existing line that this text could belong to\n",
    "#     found_line = None\n",
    "#     for s_top_y_axis, s_item in lines.items():\n",
    "#         if abs(s_top_y_axis - top_y_axis) <= y_threshold:\n",
    "#             if top_y_axis < s_item[0][1] + y_threshold:\n",
    "#                 found_line = s_top_y_axis\n",
    "#                 break\n",
    "\n",
    "#     if found_line is None:\n",
    "#         # No suitable line found, create a new line\n",
    "#         lines[top_y_axis] = [(top_y_axis, bottom_y_axis), []]\n",
    "#         found_line = top_y_axis\n",
    "#     else:\n",
    "#         # Update the bottom_y_axis if necessary\n",
    "#         _, current_bottom_y = lines[found_line][0]\n",
    "#         if bottom_y_axis > current_bottom_y:\n",
    "#             lines[found_line][0] = (top_y_axis, bottom_y_axis)\n",
    "\n",
    "#     # Add the text to the found line and track x positions\n",
    "#     lines[found_line][1].append((top_x_axis, text.description))\n",
    "#     x_positions[top_x_axis].append(text.description)\n",
    "\n",
    "# # Calculate maximum width for each x position\n",
    "# max_widths = {x: max(len(word) for word in words) for x, words in x_positions.items()}\n",
    "\n",
    "# # Sort and join the texts for each line with adjusted spacing and alignment\n",
    "# for _, item in lines.items():\n",
    "#     if item[1]:\n",
    "#         words = sorted(item[1], key=lambda t: t[0])\n",
    "#         sentence = []\n",
    "#         last_x = None\n",
    "#         for x, word in words:\n",
    "#             if last_x is not None:\n",
    "#                 space_width = int((x - last_x) / 30)  # Adjust divisor to scale space width\n",
    "#                 sentence.append(' ' * max(space_width, 1))\n",
    "#             # Pad word for alignment\n",
    "#             padded_word = word.ljust(max_widths[x])\n",
    "#             sentence.append(padded_word)\n",
    "#             last_x = x + len(padded_word) * 7  # Estimate the end x position of the current word\n",
    "\n",
    "#         items.append((item[0], ''.join(sentence), words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# items = []\n",
    "# lines = {}\n",
    "# y_threshold = 20  # Adjust this value based on average text line height and spacing\n",
    "# x_positions = defaultdict(list)\n",
    "\n",
    "# # Process text annotations to organize them into lines\n",
    "# for text in response.text_annotations[1:]:\n",
    "#     top_x_axis = text.bounding_poly.vertices[0].x\n",
    "#     top_y_axis = text.bounding_poly.vertices[0].y\n",
    "#     bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "\n",
    "#     # Determine the line by checking y-axis alignment within a threshold\n",
    "#     found_line = None\n",
    "#     for s_top_y_axis, s_item in lines.items():\n",
    "#         line_mid_y = (s_item[0][0] + s_item[0][1]) / 2\n",
    "#         if abs(line_mid_y - (top_y_axis + bottom_y_axis) / 2) <= y_threshold:\n",
    "#             found_line = s_top_y_axis\n",
    "#             break\n",
    "\n",
    "#     if found_line is None:\n",
    "#         lines[top_y_axis] = [(top_y_axis, bottom_y_axis), []]\n",
    "#         found_line = top_y_axis\n",
    "#     else:\n",
    "#         # Update the line's bounding box if necessary\n",
    "#         existing_top, existing_bottom = lines[found_line][0]\n",
    "#         new_top = min(existing_top, top_y_axis)\n",
    "#         new_bottom = max(existing_bottom, bottom_y_axis)\n",
    "#         lines[found_line][0] = (new_top, new_bottom)\n",
    "\n",
    "#     lines[found_line][1].append((top_x_axis, text.description))\n",
    "#     x_positions[top_x_axis].append(text.description)\n",
    "\n",
    "# # Calculate maximum width for each x position\n",
    "# max_widths = {x: max(len(word) for word in words) for x, words in x_positions.items()}\n",
    "\n",
    "# # Sort and join the texts for each line with adjusted spacing and alignment\n",
    "# for _, item in lines.items():\n",
    "#     if item[1]:\n",
    "#         words = sorted(item[1], key=lambda t: t[0])\n",
    "#         sentence = []\n",
    "#         last_end_x = 0  # Track the end position of the last word\n",
    "\n",
    "#         for x, word in words:\n",
    "#             if last_end_x != 0:\n",
    "#                 space_width = (x - last_end_x) // 15\n",
    "#                 sentence.append(' ' * max(space_width, 1))\n",
    "#             padded_word = word.ljust(max_widths[x])\n",
    "#             sentence.append(padded_word)\n",
    "#             last_end_x = x + len(padded_word) * 6\n",
    "\n",
    "#         items.append((item[0], ''.join(sentence), words))\n",
    "\n",
    "# # Align text starting at the same x-axis position\n",
    "# max_line_length = max(len(line[1]) for line in items)\n",
    "# aligned_text = []\n",
    "# for _, text, _ in items:\n",
    "#     aligned_text.append(text.ljust(max_line_length))\n",
    "\n",
    "# print(aligned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# items = []\n",
    "# lines = defaultdict(list)\n",
    "# y_threshold = 10  # Adjust based on the text size and line spacing\n",
    "# x_threshold = 50  # Horizontal threshold for grouping words closely\n",
    "\n",
    "# # Process text annotations to organize them into lines\n",
    "# for text in response.text_annotations[1:]:\n",
    "#     top_x_axis = text.bounding_poly.vertices[0].x\n",
    "#     top_y_axis = text.bounding_poly.vertices[0].y\n",
    "#     bottom_y_axis = text.bounding_poly.vertices[3].y\n",
    "\n",
    "#     # Determine the line by checking y-axis alignment within a threshold\n",
    "#     found_line = None\n",
    "#     for s_top_y_axis, s_item in lines.items():\n",
    "#         if abs(s_top_y_axis - top_y_axis) <= y_threshold:\n",
    "#             found_line = s_top_y_axis\n",
    "#             break\n",
    "\n",
    "#     if found_line is None:\n",
    "#         lines[top_y_axis].append((top_x_axis, text.description))\n",
    "#     else:\n",
    "#         lines[found_line].append((top_x_axis, text.description))\n",
    "\n",
    "# # Identify common x-axis starting positions\n",
    "# x_starts = defaultdict(list)\n",
    "# for y, texts in lines.items():\n",
    "#     for x, text in texts:\n",
    "#         x_starts[x].append((y, text))\n",
    "\n",
    "# # Calculate the maximum width for each x-axis start position\n",
    "# max_widths = {}\n",
    "# for x, entries in x_starts.items():\n",
    "#     max_widths[x] = max(len(text) for _, text in entries)\n",
    "\n",
    "# # Sort lines and format text with horizontal alignment\n",
    "# sorted_lines = sorted(lines.items(), key=lambda x: x[0])\n",
    "# formatted_lines = []\n",
    "# for y, texts in sorted_lines:\n",
    "#     sorted_texts = sorted(texts, key=lambda x: x[0])\n",
    "#     line_text = []\n",
    "#     last_end_x = 0\n",
    "#     for x, text in sorted_texts:\n",
    "#         if last_end_x != 0:\n",
    "#             space_width = (x - last_end_x) // 10\n",
    "#             if space_width > x_threshold // 10:\n",
    "#                 line_text.append('   ')  # Add extra space for larger gaps\n",
    "#             else:\n",
    "#                 line_text.append(' ')\n",
    "#         # Pad text for alignment\n",
    "#         padded_text = text.ljust(max_widths[x])\n",
    "#         line_text.append(padded_text)\n",
    "#         last_end_x = x + len(padded_text) * 7  # Estimate the end x position of the current word\n",
    "#     formatted_lines.append(''.join(line_text))\n",
    "\n",
    "# # Print each line with some separation\n",
    "# for line in formatted_lines:\n",
    "#     print(line)\n",
    "#     print()  # Print a blank line for separation between lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
